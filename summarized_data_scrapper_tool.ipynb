{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#google_search_scrapper_tool\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "# Ensure required modules are installed\n",
        "def install_packages():\n",
        "    packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"google-search-results\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "install_packages()\n",
        "\n",
        "from serpapi import GoogleSearch\n",
        "import pandas as pd\n",
        "\n",
        "def google_search_scraper(query, num_results=10):\n",
        "    \"\"\"\n",
        "    Fetches Google search results using SerpAPI.\n",
        "    \"\"\"\n",
        "    api_key = \"bcc39452a15e256ee9a03213aede7be60ffc66bb4e044be17296963493fef2ab\"  # Replace with your actual API key\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"num\": num_results,\n",
        "        \"api_key\": api_key\n",
        "    }\n",
        "\n",
        "    search = GoogleSearch(params)\n",
        "    results = search.get_dict()\n",
        "    search_results = []\n",
        "\n",
        "    if \"organic_results\" in results:\n",
        "        for result in results[\"organic_results\"]:\n",
        "            title = result.get(\"title\", \"No Title\")\n",
        "            link = result.get(\"link\", \"No Link\")\n",
        "            snippet = result.get(\"snippet\", \"No Description\")\n",
        "            search_results.append({\"Title\": title, \"Link\": link, \"Description\": snippet})\n",
        "    else:\n",
        "        print(\"No results found.\")\n",
        "\n",
        "    return search_results\n",
        "\n",
        "def save_to_csv(results, filename=\"google_search_results.csv\"):\n",
        "    \"\"\"\n",
        "    Saves the search results to a CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = input(\"Enter search query: \")\n",
        "    results = google_search_scraper(query)\n",
        "    if results:\n",
        "        save_to_csv(results)\n",
        "    else:\n",
        "        print(\"No results found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVmPXdREnVS",
        "outputId": "430b8763-9ca1-4be8-c6ec-74fdd06ec318"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter search query: Latest Tesla News\n",
            "Results saved to google_search_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#youyube_video_scrapper_tool\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"google-search-results\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "install_packages()\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def youtube_video_scraper(query, max_results=10):\n",
        "    \"\"\"\n",
        "    Fetches YouTube video results using SerpAPI.\n",
        "    \"\"\"\n",
        "    api_key = \"bcc39452a15e256ee9a03213aede7be60ffc66bb4e044be17296963493fef2ab\"  # Replace with your actual SerpAPI key\n",
        "    url = \"https://serpapi.com/search\"\n",
        "    params = {\n",
        "        \"engine\": \"youtube\",\n",
        "        \"search_query\": query,\n",
        "        \"api_key\": api_key,\n",
        "        \"num\": max_results\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    video_results = []\n",
        "    for item in data.get(\"video_results\", []):\n",
        "        title = item.get(\"title\", \"No Title\")\n",
        "        video_url = item.get(\"link\", \"No URL\")\n",
        "        description = item.get(\"snippet\") or item.get(\"rich_snippet\") or item.get(\"description\", \"No Description\")\n",
        "        channel_info = item.get(\"channel\", {})\n",
        "        channel_name = channel_info.get(\"name\", \"Unknown Channel\")\n",
        "        views = item.get(\"views\", \"Unknown Views\")\n",
        "\n",
        "        video_results.append({\n",
        "            \"Title\": title,\n",
        "            \"URL\": video_url,\n",
        "            \"Description\": description,\n",
        "            \"Channel Name\": channel_name,\n",
        "            \"Views\": views\n",
        "        })\n",
        "\n",
        "    return video_results\n",
        "\n",
        "def save_to_csv(results, filename=\"youtube_video_results.csv\"):\n",
        "    \"\"\"\n",
        "    Saves the YouTube video results to a CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = input(\"Enter YouTube search query: \")\n",
        "    results = youtube_video_scraper(query)\n",
        "    if results:\n",
        "        save_to_csv(results)\n",
        "    else:\n",
        "        print(\"No results found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEO9sDH4KLV5",
        "outputId": "32ea6dbb-c4b9-4800-f4db-6b4d7d0347d2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter YouTube search query: Breaking World News Today\n",
            "Results saved to youtube_video_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#summarized_data_scrapper_tool\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\"requests\", \"pandas\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "install_packages()\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def summarize_with_serpapi(query, api_key):\n",
        "    \"\"\"\n",
        "    Uses SerpAPI to fetch summarized results for a given query.\n",
        "    \"\"\"\n",
        "    url = \"https://serpapi.com/search\"\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"q\": query,\n",
        "        \"api_key\": api_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extract AI-generated answers or snippets\n",
        "    summary = data.get(\"answer_box\", {}).get(\"snippet\") or \\\n",
        "              data.get(\"organic_results\", [{}])[0].get(\"snippet\", \"No Summary Available\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "def process_csv(file_path, api_key):\n",
        "    \"\"\"\n",
        "    Reads a CSV file, queries SerpAPI for summaries, and outputs key insights.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    if \"Title\" not in df.columns:\n",
        "        print(\"No 'Title' column found.\")\n",
        "        return\n",
        "\n",
        "    df[\"Summary\"] = df[\"Title\"].apply(lambda x: summarize_with_serpapi(str(x), api_key) if pd.notna(x) else \"No Summary\")\n",
        "    output_file = \"summarized_data.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Summarized results saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file = input(\"Enter CSV file path: \")\n",
        "    api_key = input(\"Enter SerpAPI key: \")\n",
        "    process_csv(csv_file, api_key)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-2aF4UKKr4C",
        "outputId": "466c2820-cb4b-4297-8e9a-7490e56aecf0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter CSV file path: /content/google_search_results.csv\n",
            "Enter SerpAPI key: bcc39452a15e256ee9a03213aede7be60ffc66bb4e044be17296963493fef2ab\n",
            "Summarized results saved to summarized_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjnP3KuPMvbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
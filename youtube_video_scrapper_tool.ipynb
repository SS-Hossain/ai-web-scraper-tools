{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import subprocess\n",
        "\n",
        "# # Ensure required modules are installed\n",
        "# def install_packages():\n",
        "#     packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"google-search-results\"]\n",
        "#     for package in packages:\n",
        "#         try:\n",
        "#             __import__(package)\n",
        "#         except ImportError:\n",
        "#             subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# install_packages()\n",
        "\n",
        "# from serpapi import GoogleSearch\n",
        "# import pandas as pd\n",
        "\n",
        "# def google_search_scraper(query, engine=\"google\", num_results=10):\n",
        "#     api_key = \"your-serpapi-key-here\"  # Replace with your actual API key\n",
        "#     params = {\n",
        "#         \"engine\": engine,\n",
        "#         \"q\": query,\n",
        "#         \"api_key\": api_key,\n",
        "#         \"num\": num_results\n",
        "#     }\n",
        "\n",
        "#     search = GoogleSearch(params)\n",
        "#     results = search.get_dict()\n",
        "#     search_results = []\n",
        "\n",
        "#     if results.get(\"organic_results\"):\n",
        "#         seen = set()\n",
        "#         for result in results[\"organic_results\"]:\n",
        "#             title = result.get(\"title\", \"No Title\")\n",
        "#             link = result.get(\"link\", \"No Link\")\n",
        "#             snippet = result.get(\"snippet\", \"No Description\")\n",
        "#             if link and link not in seen:\n",
        "#                 seen.add(link)\n",
        "#                 search_results.append({\"Title\": title, \"Link\": link, \"Description\": snippet})\n",
        "#     else:\n",
        "#         print(\"No results found or SerpAPI response did not contain 'organic_results'.\")\n",
        "\n",
        "#     return search_results\n",
        "\n",
        "# def save_to_csv(results, filename=\"google_search_results.csv\"):\n",
        "#     df = pd.DataFrame(results)\n",
        "#     df.dropna(subset=[\"Title\", \"Link\"], inplace=True)  # Validation\n",
        "#     df.to_csv(filename, index=False)\n",
        "#     print(f\"Results saved to {filename}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     query = input(\"Enter search query: \")\n",
        "#     results = google_search_scraper(query)\n",
        "#     if results:\n",
        "#         save_to_csv(results)\n",
        "#     else:\n",
        "#         print(\"No results found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRVmPXdREnVS",
        "outputId": "17500163-5ec8-49b8-e466-5d12f246a017"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter search query: Latest Tesla News\n",
            "Results saved to google_search_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#youyube_video_scrapper_tool\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"google-search-results\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "install_packages()\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def youtube_video_scraper(query, api_key=\"your-serpapi-key-here\", max_results=10):\n",
        "    url = \"https://serpapi.com/search\"\n",
        "    params = {\n",
        "        \"engine\": \"youtube\",\n",
        "        \"search_query\": query,\n",
        "        \"api_key\": api_key,\n",
        "        \"num\": max_results\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "        return []\n",
        "\n",
        "    video_results = []\n",
        "    seen = set()\n",
        "    for item in data.get(\"video_results\", []):\n",
        "        title = item.get(\"title\", \"No Title\")\n",
        "        video_url = item.get(\"link\", \"No URL\")\n",
        "        description = item.get(\"snippet\") or item.get(\"rich_snippet\") or item.get(\"description\", \"No Description\")\n",
        "        channel_info = item.get(\"channel\", {})\n",
        "        channel_name = channel_info.get(\"name\", \"Unknown Channel\")\n",
        "        views = item.get(\"views\", \"Unknown Views\")\n",
        "\n",
        "        if video_url and video_url not in seen:\n",
        "            seen.add(video_url)\n",
        "            video_results.append({\n",
        "                \"Title\": title,\n",
        "                \"URL\": video_url,\n",
        "                \"Description\": description,\n",
        "                \"Channel Name\": channel_name,\n",
        "                \"Views\": views\n",
        "            })\n",
        "\n",
        "    return video_results\n",
        "\n",
        "def save_to_csv(results, filename=\"youtube_video_results.csv\"):\n",
        "    df = pd.DataFrame(results)\n",
        "    df.dropna(subset=[\"Title\", \"URL\"], inplace=True)  # Validation\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = input(\"Enter YouTube search query: \")\n",
        "    api_key = input(\"Enter your SerpAPI key: \")\n",
        "    results = youtube_video_scraper(query, api_key)\n",
        "    if results:\n",
        "        save_to_csv(results)\n",
        "    else:\n",
        "        print(\"No results found.\")\n"
      ],
      "metadata": {
        "id": "nSNy6C3TSQmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd48fd30-b076-4a72-e2f2-e01d3bd6ccf7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter YouTube search query: Breaking World News Today\n",
            "Enter your SerpAPI key: bcc39452a15e256ee9a03213aede7be60ffc66bb4e044be17296963493fef2ab\n",
            "Results saved to youtube_video_results.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9W8RZwMAtfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
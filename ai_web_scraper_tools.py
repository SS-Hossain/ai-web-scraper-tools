# -*- coding: utf-8 -*-
"""ai-web-scraper-tools.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qg6bNJO0Pb04vY_6R0oS92LYHtzsW6vI
"""

import sys
import subprocess

# Ensure required modules are installed
def install_packages():
    packages = ["requests", "beautifulsoup4", "pandas", "google-search-results"]
    for package in packages:
        try:
            __import__(package)
        except ImportError:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_packages()

from serpapi import GoogleSearch
import pandas as pd

def google_search_scraper(query, num_results=10):
    """
    Fetches Google search results using SerpAPI.
    """
    api_key = "bcc39452a15e256ee9a03213aede7be60ffc66bb4e044be17296963493fef2ab"  # Replace with your actual API key
    params = {
        "q": query,
        "num": num_results,
        "api_key": api_key
    }

    search = GoogleSearch(params)
    results = search.get_dict()
    search_results = []

    if "organic_results" in results:
        for result in results["organic_results"]:
            title = result.get("title", "No Title")
            link = result.get("link", "No Link")
            snippet = result.get("snippet", "No Description")
            search_results.append({"Title": title, "Link": link, "Description": snippet})
    else:
        print("No results found.")

    return search_results

def save_to_csv(results, filename="google_search_results.csv"):
    """
    Saves the search results to a CSV file.
    """
    df = pd.DataFrame(results)
    df.to_csv(filename, index=False)
    print(f"Results saved to {filename}")

if __name__ == "__main__":
    query = input("Enter search query: ")
    results = google_search_scraper(query)
    if results:
        save_to_csv(results)
    else:
        print("No results found.")

from google.colab import files

# Download the Python script (if saved in Colab)
files.download("google-search-scraper.py")


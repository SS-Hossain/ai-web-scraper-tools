import sys
import subprocess

# Ensure required modules are installed
def install_packages():
    packages = ["requests", "beautifulsoup4", "pandas", "google-search-results"]
    for package in packages:
        try:
            __import__(package)
        except ImportError:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install_packages()

from serpapi import GoogleSearch
import pandas as pd

def google_search_scraper(query, num_results=10):
    api_key = "your-serpapi-key-here"  # Replace with your actual API key
    params = {
        "engine": "google",
        "q": query,
        "api_key": api_key,
        "num": num_results
    }

    search = GoogleSearch(params)
    results = search.get_dict()
    search_results = []

    if results.get("organic_results"):
        seen = set()
        for result in results["organic_results"]:
            title = result.get("title", "No Title")
            link = result.get("link", "No Link")
            snippet = result.get("snippet", "No Description")
            if link and link not in seen:
                seen.add(link)
                search_results.append({"Title": title, "Link": link, "Description": snippet})
    else:
        print("No results found or SerpAPI response did not contain 'organic_results'.")

    return search_results

def save_to_csv(results, filename="google_search_results.csv"):
    df = pd.DataFrame(results)
    df.dropna(subset=["Title", "Link"], inplace=True)  # Validation
    df.to_csv(filename, index=False)
    print(f"Results saved to {filename}")

if __name__ == "__main__":
    query = input("Enter search query: ")
    results = google_search_scraper(query)
    if results:
        save_to_csv(results)
    else:
        print("No results found.")
